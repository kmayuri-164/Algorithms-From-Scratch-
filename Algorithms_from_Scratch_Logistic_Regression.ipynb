{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the dataset\n",
    "dataset=load_breast_cancer(as_frame=True)\n",
    "df=pd.DataFrame(data=dataset.data)\n",
    "df[\"target\"]=dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                 0.07871  ...          17.33           184.60      2019.0   \n",
       "1                 0.05667  ...          23.41           158.80      1956.0   \n",
       "2                 0.05999  ...          25.53           152.50      1709.0   \n",
       "3                 0.09744  ...          26.50            98.87       567.7   \n",
       "4                 0.05883  ...          16.67           152.20      1575.0   \n",
       "\n",
       "   worst smoothness  worst compactness  worst concavity  worst concave points  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   worst symmetry  worst fractal dimension  target  \n",
       "0          0.4601                  0.11890       0  \n",
       "1          0.2750                  0.08902       0  \n",
       "2          0.3613                  0.08758       0  \n",
       "3          0.6638                  0.17300       0  \n",
       "4          0.2364                  0.07678       0  \n",
       "\n",
       "[5 rows x 31 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting of x and y\n",
    "X=df.iloc[:,:-1]\n",
    "Y=df.iloc[:,-1]\n",
    "\n",
    "#splitting dataset into traning and test\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,Y,train_size=0.75,shuffle=True,random_state=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param_init(X):\n",
    "    \"\"\" \n",
    "    initialize paramaters\n",
    "    X: training data\n",
    "    ----------------------------\n",
    "    output : \n",
    "    parmas : dictionary contaning coeeficient\"\"\"\n",
    "    \n",
    "    parmas={}\n",
    "    _,n_features=X.shape\n",
    "    \n",
    "    parmas[\"W\"]=np.zeros(n_features)\n",
    "    parmas[\"b\"]=0\n",
    "    return parmas "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_z(X,W,b):\n",
    "    \"\"\"\n",
    "    calculate the linear fucntion \n",
    "    ---------------------------------------\n",
    "    input \n",
    "    X: traning data\n",
    "    W: weight coeeficient\n",
    "    b: bias coeeficient\n",
    "    ----------------------------------------\n",
    "    output :\n",
    "    Z: linear function \"\"\"\n",
    "    \n",
    "    z=np.dot(X,W) + b\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    \"\"\"\n",
    "    logit model\n",
    "    ------------------------------\n",
    "    input (s):\n",
    "    z :  lienar model\n",
    "    -----------------------------------\n",
    "    output :\n",
    "    g : logit function applied to linear model \"\"\"\n",
    "    \n",
    "    g=1/(1 + np.exp(-z))  \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_desent(X,y,parmas,lr,n_iter):\n",
    "    \"\"\"\n",
    "    gradient descent to minise the cost function\n",
    "    -----------------------------------------------\n",
    "    input function \n",
    "    x: training data\n",
    "    y: lables\n",
    "    params: dictinary contaning coeeficient\n",
    "    lr: lerning rate\n",
    "    --------------------------------------------\n",
    "    output :\n",
    "    parmas: dictonary conating optimized coeeficient \"\"\"\n",
    "    \n",
    "    W=parmas[\"W\"]\n",
    "    b=parmas[\"b\"]\n",
    "    m=X.shape[0]   #number of traning instaces\n",
    "    \n",
    "    for _ in range(n_iter):\n",
    "        g=sigmoid(get_z(X,W,b)) #prediction with random weight\n",
    "        loss=(-1/m)*np.sum(y*np.log(g) + (1-y)*np.log(1-g))\n",
    "        \n",
    "        dW=(1/m)*np.dot(X.T,(g-y))\n",
    "        db=(1/m)*np.sum(g-y)\n",
    "        \n",
    "        W-=lr*dW\n",
    "        b-=lr*db\n",
    "        \n",
    "    parmas[\"W\"]=W\n",
    "    parmas[\"b\"]=b\n",
    "    return parmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(X,y,lr=0.01,n_iter=1000):\n",
    "    \"\"\"\n",
    "    traning the linear regresssion model with gradient \n",
    "    ------------------------------------------------------\n",
    "    input: traning data\n",
    "    y : lables\n",
    "    lr learning rate\n",
    "    n_iter : number of iterations\n",
    "    ---------------------------------------------------------------\n",
    "    \n",
    "    output :\n",
    "    dictionary conataing optimized coeeficniet \"\"\"\n",
    "    init_params =param_init(X)\n",
    "    parmas=gradient_desent(X,y,init_params,lr,n_iter)\n",
    "    return parmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X_test,parmas):\n",
    "    \"\"\"\n",
    "    train the linear rgression model\n",
    "    -------------------------------------\n",
    "    input :\n",
    "    unseen data\n",
    "    output:\n",
    "    prediction model \"\"\"\n",
    "    \n",
    "    z=np.dot(X_test,parmas[\"W\"] ) + parmas[\"b\"]\n",
    "    y_pred=sigmoid(z) >=0.5\n",
    "    return y_pred.astype(\"int\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-16-2deb926ff43a>:11: RuntimeWarning: overflow encountered in exp\n",
      "  g=1/(1 + np.exp(-z))\n",
      "<ipython-input-17-1043b12d0231>:20: RuntimeWarning: divide by zero encountered in log\n",
      "  loss=(-1/m)*np.sum(y*np.log(g) + (1-y)*np.log(1-g))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Implementation: 0.9300699300699301\n",
      "Sklearn Implementation: 0.9300699300699301\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:762: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "params = train(X_train, y_train) # train model\n",
    "y_pred = predict(X_test, params) # inference\n",
    "lr = LogisticRegression(C=0.01)\n",
    "lr.fit(X_train, y_train)\n",
    "sklearn_y_pred = lr.predict(X_test)\n",
    "print(f\"My Implementation: {accuracy_score(y_test, y_pred)}\\nSklearn Implementation: {accuracy_score(y_test, sklearn_y_pred)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assumptions\n",
    "Binary or Ordinal â€” The response variable is required to be binary in binary logistic regression and \n",
    "ordinal in ordinal logistic regression\n",
    "Independence â€” Observations are required to be independent of each other\n",
    "Multicollinearity â€” Little to no multicollinearity among the predictor variables.\n",
    "Linearity â€” Linearity of independent variables and log-odds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros\n",
    "Low Variance\n",
    "Provides probabilities\n",
    "Easy to Implement\n",
    "Cons\n",
    "High Bias"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
